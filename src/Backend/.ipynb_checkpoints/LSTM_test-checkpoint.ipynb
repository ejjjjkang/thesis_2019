{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from io import open\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "불용어 제거할 것 \n",
    "1. 숫자\n",
    "2. 한글자\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#save json file to text\n",
    "PATH = './dataset/'\n",
    "\n",
    "commentText = [];\n",
    "\n",
    "with open(PATH + \"test_data.json\", encoding='utf-8') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    print(\"완료\")\n",
    "    for json_string in json_data:\n",
    "        commentText.append(json_string['commentText'])\n",
    "        \n",
    "str1 = ''.join(commentText)\n",
    "\n",
    "f = open(\"conservative.txt\", 'w')\n",
    "f.write(str1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        #self.okt = Okt()\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        with open(path, encoding=\"utf8\") as fred:\n",
    "            for line in fred:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                    \n",
    "        with open(path, encoding=\"utf8\") as fred:\n",
    "            idss = []\n",
    "            for line in fred:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "            return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    1,    2,  ..., 2112,   15,    0])\n",
      "33278\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "corpus = Corpus('./wiki_test.txt')\n",
    "print(corpus.tokenize('./wiki_test.txt'))\n",
    "\n",
    "print(len(corpus.dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     1,     2,  ..., 16508, 16509,     2])\n",
      "16510\n"
     ]
    }
   ],
   "source": [
    "corpus_1 = Corpus('./conservative.txt')\n",
    "print(corpus_1.tokenize('./conservative.txt'))\n",
    "\n",
    "print(len(corpus_1.dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "#model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=0\n",
    "# result = []\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     line = fread.readline() #한 줄씩 읽음.\n",
    "#     if not line: break # 모두 읽으면 while문 종료.\n",
    "#     n=n+1\n",
    "#     if n%5000==0: # 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인지 출력.\n",
    "#         print(\"%d번째 While문.\"%n)\n",
    "#     tokenlist = okt.pos(line, stem=True, norm=True) # 단어 토큰화\n",
    "#     temp=[]\n",
    "#     for word in tokenlist:\n",
    "#         if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "#             temp.append((word[0])) # 해당 단어를 저장함\n",
    "\n",
    "#     if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "#       result.append(temp) # 결과에 저장\n",
    "# fread.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
